# spider

## Purpose
The purpose of this project is to develop a web scraper designed to extract movie reviews from sites like IMDb and Rotten Tomatoes. This tool will enable efficient data collection for research, analysis, and decision-making, addressing the need for comprehensive insights into film criticism.

## Description
This project will involve building a web scraper that navigates the specified websites, extracts movie review data, and stores it in formats such as CSV, JSON, or directly in a database. The project will utilize Beautiful Soup, a Python library tailored for parsing HTML and XML documents. Key features will include:
- Configurable data extraction based on user-defined parameters
- Support for multiple output formats, including databases like SQLite or PostgreSQL.
- Error handling for common web scraping issues (e.g., broken links, access restrictions)

## New Concepts
- Advanced usage of Beautiful Soup for complex data parsing specific to movie reviews.
- Techniques for handling and storing large datasets of reviews.
- Implementing web scraping ethics, including respect for `robots.txt` and rate limiting

## Resources
- **Hardware:** Computer with internet access
- **Software:** Python, Beautiful Soup, libraries for data storage (e.g., Pandas), text editor/IDE
- **Estimated Costs:** Minimal, primarily software tools (most are free or open-source)

## Dependencies
- Installation of Python and necessary libraries (requests, Beautiful Soup, etc.)
- Access to target websites for data extraction:
-- IMDb (Internet Movie Database): IMDb
-- Rotten Tomatoes: Rotten Tomatoes
-- Metacritic: Metacritic
-- Letterboxd: Letterboxd
-- FilmAffinity: FilmAffinity
-- The Guardian Film Section: The Guardian
-- RogerEbert.com: Roger Ebert
-- NPR Movies: NPR Movies
-- Collider: Collider
-- Screen Rant: Screen Rant


## Risks
- Potential website restrictions that may prevent scraping
- Changes in website structure could affect the crawling logic
- Legal considerations regarding data scraping from specific sites

## Milestones
- **Sprint 1: Project Setup** (9/30)  
  Research best practices in web scraping and finalize project scope.
  
- **Sprint 2: Development of Basic Web Crawler** (10/14)  
  Implement basic crawling functionality using Beautiful Soup.
  
- **Sprint 3: Data Extraction Logic** (10/28)  
  Develop algorithms to extract specified data points from target websites.
  
- **Sprint 4: Data Storage Implementation** (11/11)  
  Implement data storage options (CSV, JSON, database).
  
- **Sprint 5: Testing and Debugging** (11/25)  
  Conduct thorough testing and fix identified issues.
  
- **Sprint 6: Final Presentation and Documentation** (12/9)  
  Prepare documentation and present the final project to stakeholders.


## Progress
**Total Hours:** 120

- **W1:** 0
- **W2:** 3
- **W3:** 10

### Remaining Hours
**107 hours remaining**
